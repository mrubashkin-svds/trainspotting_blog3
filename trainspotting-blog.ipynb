{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#thread management\n",
    "from threading import Thread,Lock\n",
    "from collections import deque\n",
    "\n",
    "#image manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 #OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#general modules\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#camera modules\n",
    "#import picamera as pc\n",
    "#from picamera.array import PiRGBArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Streaming Video Analysis in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "** Written By:\n",
    "** Matthew Rubashkin and Colin Higgins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "At SVDS we have [*analyzed Caltrain\n",
    "delays*](http://www.svds.com/the-trains-project-analyzing-caltrain-delays/)\n",
    "to try to improve Caltrain arrival predictions using real time publicly\n",
    "available data. However, there were some inconsistencies with the\n",
    "station arrival time data we pulled from the API. In order to increase\n",
    "the accuracy of our predictions, we needed to verify when, where, and in\n",
    "which direction trains were going. In our previous [post](http://www.svds.com/image-processing-python/), Chloe Mawer implemented a proof-of-concept Caltrain detector\n",
    "using a webcam to acquire a video at our Mountain View offices. She\n",
    "explained the use of OpenCV’s python bindings to walk through\n",
    "frame-by-frame image processing. She showed that using video alone, it\n",
    "is possible to positively identify a train based on motion from frame to\n",
    "frame. She also showed how to use regions of interest within the frame\n",
    "to determine the direction in which the Caltrain was traveling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><video width=\"960\" height=\"360\" align=\"center\" controls>\n",
    "  <source src=\"video/confusion_matrix_movie.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The previous work was done using pre-recorded, hand-selected video.\n",
    "Since our goal is to provide real time Caltrain detection, we had to\n",
    "implement a streaming train detection algorithm and measure its\n",
    "performance under real-world conditions. Thinking about a Caltrain\n",
    "detector IoT device as a product, we also needed to slim down from a\n",
    "camera + laptop to something with a smaller form factor. We already had\n",
    "some experience [*listening to\n",
    "trains*](http://www.svds.com/listening-caltrain/) using a Raspberry Pi,\n",
    "so we bought a [*camera\n",
    "module*](https://www.raspberrypi.org/products/pi-noir-camera/) for it\n",
    "and integrated our video acquisition and processing/detection pipeline\n",
    "onto one device. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Pi_Video_Only_Architecture.png\" alt=\"Smooth images\" width=\"1200\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On our Raspberry Pi 3B, our pipeline consists of hardware and software running on top of [Raspbian Jesse](https://www.raspberrypi.org/blog/raspbian-jessie-is-here/), a derivative of Debian Linux. All of the software is written in [python 2.7](https://www.python.org) and can be controlled from a [Jupyter Notebook](http://jupyter.org) run locally on the Pi or remotely on your laptop. Highlighted in green are our 3 major components for acquiring, processing, and evaluating streaming video:\n",
    "-   **Video Camera**: Initializes picamera and captures frames from the video stream\n",
    "-   **Video Sensor:** Processes the captured frames and dynamically varies video camera settings \n",
    "-   **Video Detector**: Determines motion in specifed Regions of Interst (ROIs), and evaluates if a train passed\n",
    "\n",
    "In addition to our main camera, sensor and detector processes, several sub-classes (orange) are needed to perform image background subtraction, persist data, and run models:\n",
    "-   **Mask**: Performs background subtraction on raw images, using powerful algorithims implented in OpenCV 3.0 \n",
    "-   **History**: An accessible [Pandas](http://pandas.pydata.org) dataframe that is updated in real time to persist data and faciliates SQL-like queries\n",
    "-   **Detector Worker**: Assists the video detector in evaluating image, motion and history data. This class consists of several modules (yellow) responsible for sampling frames from the video feed, plotting data and running models to determine train direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caltrain detection, at its simplest, boils down to a simple question of\n",
    "binary classification: Is there a train passing right now? Yes or no\n",
    "\n",
    "<img src=\"figures/Binary_confusion_matrix.png\" alt=\"OpenCV\" width=\"150\" align='right'>\n",
    "As with any other binary classifier, the performance is defined by\n",
    "evaluating the number of examples in each of four cases:\n",
    "1. **Classifier says there is a train and there is a train, True Positive**\n",
    "2. **Classifier says there is a train when there is none, False Positive**\n",
    "3. **Classifier says there is no train when there is one, False Negative**\n",
    "4. **Classifier says there is no train when there isn’t one, True Negative** \n",
    "\n",
    "For more info check out the blogs by Tom Fawcett, principal data scientist at\n",
    "SVDS, on [classifier evaluation](http://www.svds.com/the-basics-of-classifier-evaluation-part-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our minimum viable Caltrain detector for a week, we began\n",
    "to understand how our classifier performed, and importantly, where it\n",
    "failed.\n",
    "\n",
    "Causes of false positives:\n",
    "\n",
    "-   Delivery trucks\n",
    "-   Garbage trucks\n",
    "-   Light rail\n",
    "-   Freight trains\n",
    "\n",
    "Causes of false negatives:\n",
    "\n",
    "-   Darkness\n",
    "-   Rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier involves two main parameters set empirically, motion and time. We first evaluate the amount of motion in selected Region of Interest (ROIs). This is done at 5 frames per second. The second parameter we evaluate is motion over time, wherein a set amount of motion must occur over a certain amount of time to be considered a train. We set our time threshold at 2 seconds, since express trains take ~3 seconds to pass by our sensor located 50 feet from the tracks. As you can imagine, objects like humans walking past our IoT device will not create large enough motion to trigger a detection event, but large objects like freight trains or trucks will trigger a false positive detection event if they traverse the video sensor ROIs over 2 seconds or more. The next 2 blog posts will discuss how we integrate audio and image classification to decrease false positive events.\n",
    "\n",
    "While our video classifier works decently well at detecting trains during the day, we were unable to detect trains (false negatives) in low light conditions after sunset. When we tried additional computationally expensive image processing to detect trains in low light on the Raspberry Pi, this caused all other processes including image capture to grind to a halt! \n",
    "\n",
    "So before we dive into the data and how we solved these problems, let’s talk about some of the nuts and bolts. How do we capture video and process it on the Raspberry Pi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PiCamera and the Video_Camera Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The [*PiCamera*](https://picamera.readthedocs.io/) package is an\n",
    "open-source package that offers a pure Python interface to the Pi camera\n",
    "module that allows you to record image or video to file or stream. After\n",
    "some experimentation, we decided to use PiCamera in a [continuous capture\n",
    "mode](http://picamera.readthedocs.io/en/release-1.10/api_camera.html), as shown below in the **initialize_camera** and **initialize_video_stream** functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Video_Camera(Thread):\n",
    "    def __init__(self,fps,width,height,vflip,hflip,mins):\n",
    "        self.input_deque=deque(maxlen=fps*mins*60) \n",
    "        #...\n",
    "        \n",
    "    def initialize_camera(self):\n",
    "        self.camera = pc.PiCamera(\n",
    "            resolution=(self.width,self.height), \n",
    "            framerate=int(self.fps))\n",
    "            #...\n",
    "    \n",
    "    def initialize_video_stream(self):\n",
    "        self.rawCapture = pc.array.PiRGBArray(self.camera, size=self.camera.resolution) \n",
    "        self.stream = self.camera.capture_continuous(self.rawCapture,\n",
    "             format=\"bgr\", \n",
    "             use_video_port=True)\n",
    "        \n",
    "    def run(self):\n",
    "        #This method is run when the command start() is given to the thread\n",
    "        for f in self.stream:\n",
    "            #add frame with timestamp to input queue\n",
    "            self.input_deque.append({\n",
    "                'time':time.time(),\n",
    "                'frame_raw':f.array})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/camera_codeblock_1_folded.png\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stream of still image frame captures are output as a numpy array representation of the image into a deque, a [double-ended queue](https://en.wikipedia.org/wiki/Double-ended_queue), for future processing. We decided to use a deque because we will need to add/remove/access objects from both the front (head) and back(tail) of the deque. Moreover, we can easily constrain the maximum length of our **input_deque** with the maxlen argument. As shown below, new images are appended to the front, and old images are automatically removed from the rear if the maxlen is exceeded. The deque allows calculation of motion over several frames, and enforces a limit on the total images stored in memory. It is important to minimize the memory footprint of this application as our IoT device, the Raspberry Pi 3 only has 1 GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/deque_train_example.png\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading and task management in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, we implement our video_camera class as a new [thread](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html) using the python [threading](https://docs.python.org/2/library/threading.html) module. In order to perform real time train detection on a raspberry pi, threading is critical to ensure robust performance and minimize data loss in our asynchronous detection pipeline. This is because multiple threads within a process (our python script) share the same data space with the main thread, facilitating:\n",
    "\n",
    "-   Communication of information between threads\n",
    "\n",
    "-   Interruption of individual threads without terminating the entire application\n",
    "\n",
    "-   Most importantly, individual threads can be put to sleep (held in place) while other threads are running. This allows for nonparallel tasks to run without interruption on a single processor. \n",
    "\n",
    "<img src=\"figures/threading_diagram.png\" alt=\"OpenCV\" width=\"200\" align='right'>\n",
    "For example, imagine you are reading a book but are interrupted by a freight train rolling by your office. How would you be able to come back and continue reading from the exact place where you stopped? \n",
    "\n",
    "One way you could do this is by recording the page, line and word number. This way your execution context for reading a book are these 3 numbers! Now if your coworker is using the same technique, she can borrow the book and continue reading where she stopped before. When she is done, you can even take the book back and continue from where you were. Similar to reading a book with multiple people, or asynchronously processing video and audio signals, many tasks can share the same processor on the Raspberry Pi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Background Subtraction and the Video_Sensor Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are collecting and storing data from the PiCamera in the **input_deque**, we can create a new thread, the **video_sensor**, which asynchronously process these images independent of the video_camera thread. The job of the **video_sensor** is to determine which pixels have changed values overtime, i.e. motion. To do this, we will need to identify the background of the image, the non-moving objects in the frame which inadvertently mask motion, and the foreground of the image: i.e. the new/moving objects in the frame. After we have identified motion, we will apply a 5x5 pixel kernal filter to reduce noise in our motion measurement via the [cv2.morphologyEx](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Video_Sensor(Thread):\n",
    "    def __init__(self,video_camera,mask_type):\n",
    "        #...\n",
    "        \n",
    "    def apply_mask_and_decrease_noise(self,frame_raw):\n",
    "        #apply the background subtraction mask\n",
    "        frame_motion = self.mask.apply(frame_raw)\n",
    "        #apply morphology mask to decrease noise\n",
    "        frame_motion_output = cv2.morphologyEx(\n",
    "            frame_motion,\\\n",
    "            cv2.MORPH_OPEN,\\\n",
    "            kernel=np.ones((2,2),np.uint8))\n",
    "        return frame_motion_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/camera_codeblock_2.png\" alt=\"Smooth images\" width=\"500\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time background subtraction masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chloe [previously demonstrated](http://www.svds.com/image-processing-python/) that we could detect trains with processed video feeds that isolate motion, through a process called background subtraction, by setting thresholds for the minimum intensity and duration of motion. Since background subtraction must be applied to each frame and the Pi has only modest computational speed, we needed to streamline the algorithm to reduce computational overhead. Luckily, [OpenCV 3](http://opencv.org/opencv-3-0.html) comes with multiple [background subtraction algorithms](http://docs.opencv.org/3.1.0/db/d5c/tutorial_py_bg_subtraction.html#gsc.tab=0) that run optimized C code with convenient Python APIs including:\n",
    "\n",
    "-   [backgroundsubtractorMOG2](http://www.sciencedirect.com/science/article/pii/S0167865505003521) : A Gaussian Mixture-based Background/Foreground Segmentation Algorithm developed by Zivkovic and colleagues.  It uses a method to model each background pixel by an optimized mixture of K Gaussian distributions. The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and are more static.\n",
    "\n",
    "<img src=\"figures/knn_theory.png\" alt=\"OpenCV\" width=\"200\" align='right'>\n",
    "\n",
    "\n",
    "-   [backgrounsubtractorKNN](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding.html) : KNN involves searching for the closest match of the test data in the feature space of historical image data. In our case, we are trying to discern large regions of pixels with motion and without motion. An example of this to the right, where we try and discern which class (blue square or red triangle) the new data (green circle) belongs to by factoring in not only the closest neighbor (red triangle), but the proximity threshold of k-nearest neighbors. For instance, if k=2 then the green circle would be assigned the red triangle (the two red triangles are closest), but if k=6 then the blue square class would be assigned (the closest 6 objects are 4 blue squares and only 2 red triangles). If tuned correctly, KNN background subtraction should excel at detecting large areas of motion (i.e. a train) and should reduce detection of small areas of motion (i.e. a distant tree fluttering in the wind).\n",
    "\n",
    "We tested each and found that backgroundsubtractorKNN gave the best balance between rapid response to change and adaptability, robustly recognizing vehicle motion, while not being triggered by swaying vegetation. Moreover, the KNN method  can be improved through machine learning, and the classifer can be saved to file for repeated use. The cons of KNN include artifacts from full field motion, limited tutorials, incomplete documentation, and that backgroundsubtractorKNN requires OpenCV 3.0 and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mask():\n",
    "    def __init__(self,fps):\n",
    "        #...\n",
    "        \n",
    "    def make_KNN_mask(self,bgsKNN_history,bgsKNN_d2T,bgsKNN_dS):\n",
    "        mask = cv2.createBackgroundSubtractorKNN(\\\n",
    "            history=bgsKNN_history,\\\n",
    "            dist2Threshold=bgsKNN_d2T,\\\n",
    "            detectShadows=bgsKNN_dS)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/camera_codeblock_3.png\" alt=\"Smooth images\" width=\"600\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><video width=\"960\" height=\"360\" align=\"center\" controls>\n",
    "  <source src=\"video/mog2_knn_comparison.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically update camera settings in response to varied lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PiCamera does a great job at adjusting its brightness settings throughout the day, but struggles with limited illumination at night or during the rare downpour in Mountain View. Below you can see the motion we detected from our sensor over 24 hours, where the spikes correspond to moving objects like a CalTrain! Now if we were using a digital camera or phone, we could manually change the exposure time or turn on a flash to increase the motion we could capture post sunset or before the sunrise. However, with an automated IoT device, we must dynamically update the camera settings in response to varied lighting. We also picked a [night-vision compatible camera](https://www.raspberrypi.org/products/pi-noir-camera-v2/) without an infared (IR) filter to gather more light in the ~700-1000 nm range, where normal cameras only capture light from ~400-700nm.  This extra far to infared light is why some of our pictures seem discoloured compared to traditional cameras like your smart phone that have an IR filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/sunrise_sunset_data.jpg\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know when to change the camera settings we record the intensity mean of the image, which the camera tries to keep around 50% max levels at all times (half max = 128, i.e. half of the 8 bit 0-255 limit). We observed that when the sunset light dropped beneath ~1/16 of max, and we were unable to reliably detect motion. For this reason we set a low intensity level of 1/8 max intensity to trigger the camera night model, and we set the intensity threshold to 7/8 max intensity to trigger the day mode. We also do not continually trigger the night settings if it is already night, by checking the camera operating mode.\n",
    "\n",
    "After we change the camera settings, we reset the background subtraction mask to ensure that we do not falsely trigger train detection. Importantly, we wait 1 second between setting camera settings and triggering the mask, to ensure the camera thread is not lagging and has updated before the mask is reset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Video_Sensor(Thread):\n",
    "    #...\n",
    "    def vary_camera_settings(self,frame_raw):\n",
    "            intensity_mean=frame_raw.ravel().mean() #8 bit camera\n",
    "            #adjust camera properties dynamically if needed, then reset mask\n",
    "            if ((intensity_mean < (255.0/8) ) & (self.camera.operating_mode=='day')):\n",
    "                self.video_camera.apply_camera_night_settings()\n",
    "                time.sleep(1)\n",
    "                self.mask=self.mask_object.make_mask(self.mask_type)\n",
    "                print 'Day Mode Activated - Camera'\n",
    "            if ((intensity_mean < (255.0*(3/4)) ) & (self.camera.operating_mode=='night')):\n",
    "                self.video_camera.apply_camera_day_settings()\n",
    "                #...\n",
    "            return intensity_mean,self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/camera_codeblock_4.png\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time detection of trains with the Video_Detector Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the video sensor is recording motion in a frame 5 times a second (5 FPS), we need to create a Video_Detector for detecting how long and what direction an object has been moving through the frame. In order to do this, we create 3 Regions Of Interest (ROIs) in our frame which the train passes through. By having three ROIs, we can see if a train enters from the left (northbound) or right (southbound). We found that having a third center ROI decreases the effect of noise in an individual ROI; thereby improving our ability to predict train directionality and more accurately calculate speed.\n",
    "\n",
    "We next create [circular buffer](https://en.wikipedia.org/wiki/Circular_buffer) to store when individual ROIs have exceeded the motion threshold. The length of this motion_detected_buffer is set as the minimum time corresponding to a train, multiplied by the camera FPS (we set this as 2 seconds, i.e. the motion_detected_buffer has a length of 10). We added logic to our Video_Detector class that prevents a train from being detected more than once in cooldown period, to prevent slow moving trains as being registered as a train more than one time. Additionally, we use a frame sampling buffer to keep a short term record of raw and processed frames for future analysis, plotting or saving. \n",
    "\n",
    "Using all of these buffers, the Video_Detector class creates ROI_to_process, which is an array of information which stores time, the motion from the 3 ROIs, if motion was detected, and the direction of the train motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Video_Detector(Thread):\n",
    "    def __init__(self,video_camera,video_sensor,\\\n",
    "                 motion_threshold,time_threshold,cooldown_period):\n",
    "        \n",
    "    def create_rois(self):\n",
    "        #hardcode Regions Of Interest, ROI: ((x1, y1), (x2, y2))\n",
    "        left_roi=((2,80),(50,135))\n",
    "        center_roi=((145,90),(215,130))\n",
    "        right_roi=((325,100),(380,130))\n",
    "        self.all_rois=[left_roi,center_roi,right_roi]\n",
    "    \n",
    "    def create_buffers(self):\n",
    "        self.train_detected_buffer=deque(maxlen=self.cooldown_period)\n",
    "        self.train_direction_buffer=deque(maxlen=self.cooldown_period)\n",
    "        #length of motion_detected_buffer determines how time of motion translates into detection\n",
    "        self.motion_detected_buffer=deque(maxlen=self.time_threshold)\n",
    "        #prefill these buffers to max length\n",
    "        for i in range(0,self.cooldown_period):\n",
    "            self.motion_detected_buffer.append(0)\n",
    "            self.train_detected_buffer.append(0)\n",
    "            self.train_direction_buffer.append(0)\n",
    "        #create frame sampler buffer (do not prefill this buffer)\n",
    "        self.frame_sampler_buffer=deque(maxlen=self.cooldown_period)\n",
    "        \n",
    "    def run(self):\n",
    "        self.framenum=0\n",
    "        while self.kill_all_threads!=True:\n",
    "            data=self.output_deque.popleft()\n",
    "            #...\n",
    "            #update the history dataframe and adjust the frame number pointer\n",
    "            self.history.iloc[self.framenum % self.history.shape[0]] = self.roi_data\n",
    "            self.framenum+=1\n",
    "            #add frames and data to sampler\n",
    "            self.frame_sampler_buffer.append({\n",
    "                    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/video_detector_and_buffers_codeblock.png\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/roi_to_process_code.png\" alt=\"Smooth images\" width=\"800\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist proceesed data to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are storing relevant train sensor and detector data in memory, we use [pandas dataframes](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) to persist this data for future analysis. Pandas is a python package that provides fast and flexible data structures designed to work efficiently with both relational and labeled data. Similar to using SQL (structured query language) for managing data held in relational database management systems (RDBMS), pandas is an excellent tool for data analysis, that makes importing, querying and exporting data easy. \n",
    "\n",
    "The History class is used to create a pandas dataframe that loads time, sensor and processed detector data. As the raspberry pi 3 has limited memory (1 GB), we implement the History pandas dataframe as a limited length circular buffer to prevent memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History():\n",
    "    def __init__(self,fps):\n",
    "        self.len_history=int(fps*600)\n",
    "        self.columns=['time','left_roi','middle_roi','right_roi',\\\n",
    "                 'motion_detected','train_detected','direction'] \n",
    "        \n",
    "    def setup_history(self):\n",
    "        #create pandas dataframe that contains  column information\n",
    "        self.history = pd.DataFrame.from_records(\\\n",
    "            np.zeros((self.len_history,len(self.columns))),\n",
    "            index=np.arange(self.len_history),\n",
    "            columns=self.columns)\n",
    "        #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/history_codeblock_5.png\" alt=\"Smooth images\" width=\"650\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using pandas SQL-like commands, we can now easily retrieve and analyze train detection events. Shown below is 17 frames (3.4 seconds of data at 5 FPS) of data of a northbound Caltrain!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/history_dataframe_example.png\" alt=\"Smooth images\" width=\"600\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector_Worker Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are persisting data in a pandas dataframe, we want to be able to visualize the raw sensor and processed detector data. This requires additional processing time and resources, and we do not want to interrupt the video detector. We therefore use threading to create the Detector_Worker class. The Detector_Worker is responsible for plotting video, determing train direction and returning sampled frames to the jupyter console or filesystem. Shown below is the output of the video plotter. On the top left is one raw frame of video, and on the bottom right is one KNN-background-subtracted motion frame. The two right frames have the three ROIs overlaid onto the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Frame_Sampler_Example_Pics_Only.png\" alt=\"Smooth images\" width=\"650\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accurately detect train direction, the three of us on the Trainspotting team all tried different methods.\n",
    "\n",
    "-    Static ‘Boolean’ Method (Chloe): Track motion level in each individual ROIs and then select north/south depending on which ROI exceeded the threshold first. We found that this static boolean method does not work well for express trains which triggered the north and south facing detectors simultaneously.\n",
    "\n",
    "\n",
    "-    Streaming ‘Integration’ Method (Colin): This method involved summing the historical levels motion in each ROI, and determining direction by which ROI had the highest sum. We found that this method was too reliant on accurate setting of  ROI position, and broke down if the camera was ever moved.\n",
    "\n",
    "-    Streaming ‘Curve-Fit’ Method (Matt R) - We next tried to combine the boolean and integration method with a simple [sigmoid model](https://en.wikipedia.org/wiki/Sigmoid_function) of motion across the frame. If average motion across the three ROIs exceeded our motion threshold, we  empirically fit a sigmoid curve where the ROI sensor hits 50% of the max value. If the data was noisy and curve fitting failed, we revert back to Chloe's static boolean method. Moreover, our curve-fit method  allows determination of train speed if the real distance between the ROIs is known!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Detector_Worker(Thread):\n",
    "    def curve_func(self,x, a, b,c):\n",
    "        #Sigmoid function\n",
    "        return -a/(c+ np.exp(b * -x))\n",
    "    \n",
    "    #...\n",
    "    \n",
    "    def alternate_km_map(self,ydata,t,event_time):\n",
    "        #determine emperically where the ROI sensor hits 50% of the max value\n",
    "        max_value = max(ydata)\n",
    "        for i in range(0,len(ydata)):\n",
    "            #if the value is above half of the max value\n",
    "            if ydata[i] > max_value/2.0:\n",
    "                km=t[i]-event_time\n",
    "                return km\n",
    "        #if the value never exceeds half of the max value\n",
    "        #return the end of the time series\n",
    "        km=t[-1]-event_time\n",
    "        return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Direction_Detection_Code_Small.png\" alt=\"OpenCV\" width=\"300\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Direction_Detection_Code_Big.png\" alt=\"Smooth images\" width=\"650\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a local southbound train passing our Mountain View office. If you'd like to learn more about analyzing time series data, please see our colleague Tom Fawcett's [blog post on avoiding commond mistakes with time series data](http://www.svds.com/avoiding-common-mistakes-with-time-series/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Direction_Detection.png\" alt=\"Smooth images\" width=\"650\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determination of train speed will be covered in *Streaming Audio Analysis and IoT Sensor Fusion*. Importantly other false positives like light rails or large trucks that pass in front of the camera also trigger the sensor. By having a secondary data feed, i.e. audio, we can have a second input to determine if a train is passing by both visual and sound cues. Later in the trainspotting series we will also cover how to reduce false positive of freight trains using image recognition via *TensorFlow and Neural Nets for Recognizing Images on a Raspberry Pi*\n",
    "\n",
    "We hope that you now understand how to design your own architecture for stream video processing on an IOT device. [Trainspotting blog](http://www.svds.com/introduction-to-trainspotting/) posts including *Connecting an IoT device to the Cloud* and *How to Build a Deployable IoT Device using a Raspberry Pi* will cover using a remote server to control a Raspberry Pi in greater detail! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
