{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import urllib\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "colors = ['#0055A7', '#2C3E4F', '#26C5ED', '#00cc66', '#D34100', '#FF9700', '#091D32']\n",
    "mpl_update = {'font.size':16,'xtick.labelsize':14,'ytick.labelsize':14,'figure.figsize':[12.0,8.0],\n",
    "              'axes.labelsize':20,'axes.labelcolor':'#677385',\n",
    "              'axes.titlesize':20,'lines.color':'#0055A7','lines.linewidth':3,'text.color':'#677385'}\n",
    "mpl.rcParams.update(mpl_update)\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/svds.png\" alt=\"SVDS\" width=\"100\" align=\"right\">\n",
    "\n",
    "# Trainspotting: real-time detection of a train’s passing from video\n",
    "\n",
    "### PyCon 2016\n",
    "\n",
    "### Chloe Mawer | @chloemawer | chloe@svds.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/caltrain_header.jpg\" alt=\"CaltrainHeader\" width=\"960\" height=\"200\">\n",
    "## The Caltrain obsession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Caltrain facts\n",
    "* Commuter rail between San Francisco and San Mateo and Santa Clara counties ~30 stations \n",
    "* 118 passenger cars\n",
    "* 60% >=30 years old\n",
    "* 2014 weekday ridership is 52,019 people\n",
    "* No reliable real-time status information\n",
    "    * API outage between April 5th and June 2nd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## SVDS objective\n",
    "* Create a mobile app that provides useful real-time system status and prediction information to commuters:\n",
    "* Use publically available signals to build system that contains the state of train locations:\n",
    "    * Caltrain API\n",
    "    * Twitter\n",
    "    * Rider GPS\n",
    "* Use HQ’s proximity to Caltrain to directly observe system via audio & video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"figures/appdemo.gif\" alt=\"AppScreenShots\" width=\"350\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"figures/caltrain_tweet.jpg\" alt=\"Tweet\" width=\"425\" style=\"horizontal-align:middle\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<img src=\"figures/caltrain-sign.jpg\" alt=\"Window\" align=\"right\" width=\"400\">\n",
    "<br />\n",
    "\n",
    "# Did the train really leave?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<img src=\"figures/train_in_window.jpg\" alt=\"Window\" align=\"right\" width=\"400\">\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "# Did the train really leave?\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "<img src=\"figures/talk-roadmap.png\" alt=\"Window\" align=\"left\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/opencv.png\" alt=\"OpenCV\" width=\"50\" align='right'>\n",
    "# Motion detection in Python: [OpenCV](http://www.opencv.org)\n",
    "* Written in optimized C/C++.\n",
    "* C++, C, Python and Java interfaces.\n",
    "* Windows, Linux, Mac OS, iOS and Android.\n",
    "* Free for academic and commercial use.\n",
    "* Can use multi-core processing.\n",
    "* Designed for computational efficiency with a strong focus on real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* To install, use Anaconda!\n",
    "    * `conda install -c https://conda.binstar.org/menpo opencv3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If you are not obsessed with trains... \n",
    "* Many other applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Following along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "video_file = cv2.VideoCapture('video/orig.mp4')\n",
    "\n",
    "# Read iterates through the frames in the video object and returns:\n",
    "# 1. Logical - True if a frame has been read\n",
    "# 2. Image - an array with the current frame\n",
    "read_file, frame = video_file.read()\n",
    "original = []\n",
    "while read_file:\n",
    "    # Going to grab the frame and create a list for future use \n",
    "    original.append(frame)\n",
    "    \n",
    "    # Use imshow to play video\n",
    "    cv2.imshow('Original video',frame)\n",
    "    \n",
    "    # Get next frame \n",
    "    read_file, frame = video_file.read()\n",
    "    \n",
    "    # Pause and allow for \"q\" button to stop video\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Original video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# In the case, openCV does not want to open your avi file: \n",
    "a = []\n",
    "b = []\n",
    "original = []\n",
    "with open('video/test.avi') as f:\n",
    "    bytes = f.read()\n",
    "p = re.compile(r'\\xff\\xd8')\n",
    "p2 = re.compile(r'\\xff\\xd9')\n",
    "for m in p.finditer(bytes):\n",
    "    a += [m.start()]\n",
    "for m in p2.finditer(bytes):\n",
    "    b += [m.start()]\n",
    "for c, d in zip(a, b):\n",
    "    jpg = bytes[c:d+2]\n",
    "    original.append(cv2.imdecode(np.fromstring(jpg, dtype=np.uint8),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    cv2.imshow('Original video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Original video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Key\n",
    "* **`frame`**: An image represented by a *M* x *N* x 3 numpy ndarray.\n",
    "* **`original`**: A list containing a series of frames that make up the video. \n",
    "* **`cv2.imshow()`**: Displays an input frame. \n",
    "* **`cv2.waitKey()`**: Used to introduce delay between frames.\n",
    "* **`cv2.destroyWindow()`**: Will close a given window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>The original video</h1></center>\n",
    "<center><video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video/orig.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If you are not obsessed with trains...\n",
    "Get immediate feedback from your computer's camera! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed = cv2.VideoCapture(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will read and display the feed in a window called 'Me' until you press `q` to quit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feed.read()[0] is True unless there is no frame to grab (e.g. camera not working)\n",
    "while feed.read()[0]:\n",
    "    # Grab the current frame\n",
    "    current_frame = feed.read()[1]\n",
    "    \n",
    "    # Show the current frame in a window called \"Me\"\n",
    "    cv2.imshow('Me', current_frame) \n",
    "    \n",
    "    # Pauses to make computer time = real time\n",
    "    # and allows pressing \"q\" on the keyboard\n",
    "    # to break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must close the window with the following command when done: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyWindow('Me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discontinue the feed capturing video, you must release it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steps for train detection\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>Is something moving? </li>\n",
    "  <li>Is that moving thing a train? </li>\n",
    "  <li>In what direction is it moving? </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Is something moving? \n",
    "\n",
    "A. Develop a model of the background. <br>\n",
    "B. Identify pixels in the current frame that do not match the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A. Develop a model of the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convert to one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for j,frame in enumerate(original):\n",
    "    \n",
    "    # Convert the frame to one channel (e.g. gray scale)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow('Gray scale',gray_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Gray scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Convert to one channel</h1></center>\n",
    "<center><video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video/gray.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computer Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    current_frame = feed.read()[1]\n",
    "    gray_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('Gray scale',gray_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Gray scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Smooth the image\n",
    "\n",
    "Gaussian Blur\n",
    "<br />\n",
    "<img src=\"figures/2dgaussian.png\" alt=\"2D Gaussian\" width=\"250\" align=\"left\">\n",
    "<img src=\"figures/gaussian-kernel.png\" alt=\"Gaussian kernel\" width=\"250\" align=\"right\">\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "G(x,y) = Ae^{\\frac{-(x-\\mu_x)^2}{2\\sigma_x^{2}}+\\frac{-(y-\\mu_y)^2}{2\\sigma_y^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "Implement in Python: \n",
    "\n",
    "`smooth_frame = cv2.GaussianBlur(gray_frame, (kx, ky), 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize=(16,8))\n",
    "frame = original[20]\n",
    "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "plt.subplot(2,3,1);plt.imshow(gray_frame,cmap='Greys_r'); plt.axis('off'); plt.title('No smoothing');\n",
    "\n",
    "smooth9 = cv2.GaussianBlur(gray_frame, (9,9), 0)\n",
    "plt.subplot(2,3,2);plt.imshow(smooth9,cmap='Greys_r'); plt.axis('off'); plt.title('$k_x = k_y = 9$');\n",
    "\n",
    "smooth15 = cv2.GaussianBlur(gray_frame, (15, 15), 0)\n",
    "plt.subplot(2,3,3);plt.imshow(smooth15,cmap='Greys_r'); plt.axis('off'); plt.title('$k_x = k_y = 15$');\n",
    "\n",
    "smooth31 = cv2.GaussianBlur(gray_frame, (31, 31), 0)\n",
    "plt.subplot(2,3,4);plt.imshow(smooth31,cmap='Greys_r'); plt.axis('off'); plt.title('$k_x = k_y = 31$');\n",
    "\n",
    "smooth45 = cv2.GaussianBlur(gray_frame, (45,45), 0)\n",
    "plt.subplot(2,3,5);plt.imshow(smooth45,cmap='Greys_r'); plt.axis('off'); plt.title('$k_x = k_y = 45$');\n",
    "\n",
    "smooth61 = cv2.GaussianBlur(gray_frame, (61, 61), 0)\n",
    "plt.subplot(2,3,6);plt.imshow(smooth61,cmap='Greys_r'); plt.axis('off'); plt.title('$k_x = k_y = 61$');\n",
    "plt.savefig('figures/smooth_images_k.jpg')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Smooth the image</h1></center>\n",
    "\n",
    "<img src=\"figures/smooth_images_k.jpg\" alt=\"Smooth images\" width=\"960\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Smooth the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "k = 31 # Define Gaussian kernel size\n",
    "\n",
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply a Gaussian blur to the gray scale frame \n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    cv2.imshow('Smooth', smooth_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "        \n",
    "cv2.destroyWindow('Smooth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computer camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 31\n",
    "while True:\n",
    "    current_frame = feed.read()[1]\n",
    "    gray_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    cv2.imshow('Smooth',smooth_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cv2.destroyWindow('Smooth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Smooth the image</h1></center>\n",
    "\n",
    "<center><video width=\"640\" height=\"480\" align=\"center\" controls>\n",
    "  <source src=\"video/smooth.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculate the running average\n",
    "\n",
    "At each time step, *t*, calculate at each pixel (x,y): \n",
    "\\begin{equation}\n",
    "\\mathbf{R}(x,y,t) = (1-\\alpha)\\mathbf{R}(x,y,t-1) + \\alpha\\mathbf{F}(x,y,t)\n",
    "\\end{equation}\n",
    "\n",
    "* $\\mathbf{R}:$  Running average \n",
    "* $\\mathbf{F}:$ Frame being added to the running average\n",
    "* $\\alpha:$ How heavily to weight the new frame\n",
    "\n",
    "Implement in Python: \n",
    "\n",
    "```cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Running average, *R* at *t* is the weighted sum of the running average at *t-1* and the current frame, *F*. \n",
    "* Smoothing in time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculate the running average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.2 # Define weighting coefficient\n",
    "running_avg = None # Define variable for running average\n",
    "\n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    # If this is the first frame, making running avg current frame\n",
    "    # Otherwise, update running average with new smooth frame\n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    else:\n",
    "        cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    cv2.imshow('Running average', cv2.convertScaleAbs(running_avg))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Running average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computer camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.02 # Define weighting coefficient\n",
    "running_avg = None # Define variable for running average\n",
    "\n",
    "k = 31\n",
    "while True:\n",
    "    current_frame = feed.read()[1]\n",
    "    gray_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    # If this is the first frame, making running avg current frame\n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    else:\n",
    "        cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "        \n",
    "    cv2.imshow('Running Average', cv2.convertScaleAbs(running_avg))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyWindow('Running Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Calculate the running average</h1></center>\n",
    "<center><video width=\"640\" height=\"480\" align=\"center\" controls>\n",
    "  <source src=\"video/running.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# B. Identify pixels in the current frame that do not match the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculate the difference \n",
    "<img src=\"figures/bg-fg.jpg\" alt=\"Background and foreground\" width=\"860\">\n",
    "Implement in Python: \n",
    "\n",
    "`diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "plt.subplot(1,2,1);\n",
    "plt.imshow(original[70],cmap='Greys_r'); plt.axis('off'); \n",
    "plt.subplot(1,2,2);\n",
    "plt.imshow(original[73],cmap='Greys_r'); plt.axis('off');  \n",
    "plt.savefig('bg-fg.jpg'); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculate the difference\n",
    "<img src=\"figures/diffcolor73.png\" alt=\"Difference frame\" width=\"760\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculate the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.2 \n",
    "running_avg = None \n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    # If this is the first frame, making running avg current frame\n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "        \n",
    "    # Find |difference| between current smoothed frame and running average\n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    \n",
    "    # Then add current frame to running average after\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "\n",
    "#     cv2.imshow('Difference', diff)\n",
    "    plt.imshow(diff); plt.axis('off'); plt.tight_layout();\n",
    "    plt.savefig('figures/color-diff'+str(j).zfill(3)+'.jpg',dpi=640.0/12)\n",
    "\n",
    "    plt.imshow(frame); plt.axis('off'); plt.tight_layout();\n",
    "    plt.savefig('figures/orig-frame'+str(j).zfill(3)+'.jpg',dpi=640.0/12)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "#         break\n",
    "\n",
    "# cv2.destroyWindow('Difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computer camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.02 # Define weighting coefficient\n",
    "running_avg = None # Define variable for running average\n",
    "\n",
    "k = 31\n",
    "while True:\n",
    "    current_frame = feed.read()[1]\n",
    "    gray_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    # If this is the first frame, making running avg current frame\n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    # Find absolute difference between the current smoothed frame and the running average\n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    \n",
    "    # Then add current frame to running average after\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "        \n",
    "    cv2.imshow('Difference', diff)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Calculate the difference</h1><center>\n",
    "<center><video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video/colordiff.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Threshold the difference\n",
    "\n",
    "Deciding what constitutes motion and what is just noise.\n",
    "<img src=\"figures/diffcolor73.png\" alt=\"Difference frame\" width=\"960\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Threshold the difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "thresh = 35 # All pixels with differences above this value will be set to 1\n",
    "\n",
    "alpha = 0.2\n",
    "running_avg = None \n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    # For all pixels with a difference > thresh, turn pixel to 1, otherwise 0\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    cv2.imshow('Thresholded difference', subtracted)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyWindow('Thresholded difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Computer camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.02 # Define weighting coefficient\n",
    "running_avg = None # Define variable for running average\n",
    "\n",
    "k = 31\n",
    "while True:\n",
    "    current_frame = feed.read()[1]\n",
    "    gray_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    # If this is the first frame, making running avg current frame\n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    # Find absolute difference between the current smoothed frame and the running average\n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    \n",
    "    # Then add current frame to running average after\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    # For all pixels with a difference > thresh, turn pixel to 255, otherwise 0\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    cv2.imshow('Difference', subtracted)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Threshold the difference</h1></center>\n",
    "<center><video width=\"640\" height=\"480\" align=\"center\" controls>\n",
    "  <source src=\"video/thresh.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Is that moving thing a train? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "motion_fractions = []\n",
    "\n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "running_avg = None \n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k,k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Calculate the fraction of pixels where motion is detected\n",
    "    # i.e. where subtracted==1\n",
    "    motion_fraction = (sum(sum(subtracted))/\n",
    "                       (subtracted.shape[0]*subtracted.shape[1]))\n",
    "\n",
    "    motion_fractions.append(motion_fraction)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br />\n",
    "<center><video width=\"960\" height=\"460\" align=\"center\" controls>\n",
    "  <source src=\"video/thresh-motion.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Is that moving thing a train? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(motion_fractions);\n",
    "plt.xlabel('Frame number');\n",
    "plt.ylabel('Fraction of frame in motion');\n",
    "plt.title('Fraction of frame in motion over time');\n",
    "plt.savefig('figures/red-frame-fraction.jpg');\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/frame-fraction.jpg\" alt=\"Fraction in motion\" width=\"860\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Is that moving thing a train? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Is enough of the frame in motion? Does this motion last as long as a train? \n",
    "* Area threshold\n",
    "* Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(motion_fractions);\n",
    "plt.axhline(y=0.025,color=colors[3]);\n",
    "plt.xlabel('Frame number');\n",
    "plt.ylabel('Fraction of frame in motion');\n",
    "plt.title('Fraction of frame in motion over time');\n",
    "plt.savefig('figures/green-frame-fraction-with-horizontal.jpg');\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/green-frame-fraction-with-horizontal.jpg\" alt=\"Area threshold\" width=\"860\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Is that moving thing a train? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(motion_fractions);\n",
    "plt.axhline(y=0.025,color=colors[3]);\n",
    "plt.axvline(x=71,color=colors[3], linestyle='--');\n",
    "plt.axvline(x=136,color=colors[3], linestyle='--');\n",
    "plt.xlabel('Frame number');\n",
    "plt.ylabel('Fraction of frame in motion');\n",
    "plt.title('Fraction of frame in motion over time');\n",
    "plt.savefig('figures/green-frame-fraction-with-horizontal-and-vertical.jpg');\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/green-frame-fraction-with-horizontal-and-vertical.jpg\" alt=\"Area threshold\" width=\"860\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2. Is that moving thing a train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "history_length = 50\n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "\n",
    "frame_thresh = 0.025\n",
    "running_avg = None \n",
    "history = pd.DataFrame(data=[[0,0]],columns=['Fraction','In_motion'])\n",
    "for j, frame in enumerate(original):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (g,g), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    motion_fraction = (sum(sum(subtracted))/\n",
    "                       (subtracted.shape[0]*subtracted.shape[1]))\n",
    "    \n",
    "    history.loc[len(history)+1] = [motion_fraction, motion_fraction>frame_thresh]\n",
    "    \n",
    "    detect = (history.tail(history_length).In_motion.sum()==history_length)\n",
    "    \n",
    "    if detect: \n",
    "        print 'Train detected beginning at frame', j-history_length\n",
    "        # Reset history\n",
    "        history = pd.DataFrame(data=[[0,0]],columns=['Fraction','In_motion']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(12,9))\n",
    "plt.imshow(original[73]); plt.axis('off');\n",
    "plt.savefig('figures/original73.jpg',dpi=640.0/12);\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. In what direction is it moving? \n",
    "\n",
    "<img src=\"figures/original73.jpg\" alt=\"Area threshold\" width=\"860\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define regions of interest (ROI)\n",
    "<img src=\"figures/roiframe073.jpg\" alt=\"Area threshold\" width=\"860\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><video width=\"760\" align=\"center\" controls>\n",
    "  <source src=\"video/rois.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Track activity in ROIs separately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Make frames for video of fractions of left and right ROIs in motion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "plt.xlim([0, 160]);\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlabel('Frame number'); \n",
    "plt.ylabel('Fraction of ROI in motion');\n",
    "plt.title('Fraction of ROIs in motion over time');\n",
    "\n",
    "left_fractions = []\n",
    "right_fractions = []\n",
    "\n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "\n",
    "running_avg = None \n",
    "for j, frame in enumerate(original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    # For all pixels with a difference > thresh, turn pixel to 1, otherwise 0\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "    left_area = subtracted[270:400,240:340] \n",
    "    right_area = subtracted[270:400,540:640] \n",
    "    \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    left_fractions.append(left_fraction)\n",
    "    right_fractions.append(right_fraction)\n",
    "    \n",
    "    plt.plot(range(len(left_fractions)), left_fractions, color=colors[3], label='Left ROI')\n",
    "    plt.plot(range(len(right_fractions)), right_fractions, color=colors[0], label='Right ROI')\n",
    "    if j==0:\n",
    "        plt.legend();\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.savefig('figures/gree-left-and-right'+str(j).zfill(3)+'.jpg',dpi=640.0/12)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Make frames for video of train with ROIs outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "left_fractions = []\n",
    "right_fractions = []\n",
    "\n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "\n",
    "running_avg = None \n",
    "for j, frame in enumerate(original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    \n",
    "    # For all pixels with a difference > thresh, turn pixel to 1, otherwise 0\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "    left_area = subtracted[270:400,240:340] \n",
    "    right_area = subtracted[270:400,540:640] \n",
    "    \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    left_fractions.append(left_fraction)\n",
    "    right_fractions.append(right_fraction)\n",
    "    \n",
    "    cv2.rectangle(frame, (240,270), (340,400), (0, 204,102), thickness=2)\n",
    "    cv2.rectangle(frame, (540,270), (640,400), (0, 85,167), thickness=2)\n",
    "    \n",
    "    ax, fig = plt.subplots(figsize=(12,9))\n",
    "    plt.imshow(frame); plt.axis('off');\n",
    "    plt.savefig('figures/roiframe'+str(j).zfill(3)+'.jpg',dpi=640.0/12)\n",
    "    plt.close()\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Make final figure for fraction of ROIs in motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "plt.xlim([0, 160]);\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlabel('Frame number'); \n",
    "plt.ylabel('Fraction of ROI in motion');\n",
    "plt.title('Fraction of ROIs in motion over time');\n",
    "plt.plot(left_fractions, color=colors[3], label='Left ROI');\n",
    "plt.plot(right_fractions, color=colors[0], label='Right ROI');\n",
    "plt.savefig('figures/left-and-right.jpg')\n",
    "plt.legend();\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Track activity in ROIs separately (video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Track activity in ROIs separately</h1></center>\n",
    "<center><video width=\"1260\" height=\"460\" align=\"center\" controls>\n",
    "  <source src=\"video/green-rois-plus-plot.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Real-time direction detection</h1></center>\n",
    "<img src=\"figures/left-and-right.jpg\" alt=\"Fraction in motion\" width=\"860\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real-time direction detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize a dataframe, `history`.\n",
    "2. At each frame, *t*, add row to dataframe indicating if left and/or right meets motion threshold \n",
    "3. Repeat #2 until motion threshold met on either side for last *T* frames.\n",
    "3. The side with the detection for all *T* indicates the train's direction.\n",
    "4. Reinitialize the `history` dataframe to detect future trains.\n",
    "5. Return to #2     \n",
    "<img src=\"figures/left-and-right.jpg\" alt=\"Area threshold\" width=\"520\" align='right'>\n",
    "<img src=\"figures/train-history.png\" alt=\"Area threshold\" width=\"200\" align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Final algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "history_length = 40\n",
    "area_thresh = 0.05\n",
    "history = pd.DataFrame(columns=['Detected left','Detected right'])\n",
    "left_fractions = []\n",
    "right_fractions = []\n",
    "running_avg = None \n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "    left_area = subtracted[270:400,240:340] \n",
    "    right_area = subtracted[270:400,540:640] \n",
    "    \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    left_fractions.append(left_fraction)\n",
    "    right_fractions.append(right_fraction)\n",
    "    \n",
    "    # Update the history with whether the train detection criteria was met on either side\n",
    "    history.loc[len(history)+1] = [left_fraction>area_thresh, right_fraction>area_thresh]\n",
    "    \n",
    "    # Get how many of last n frames had a train detected for left and right ROIs\n",
    "    left_cum, right_cum = history.tail(history_length).sum()\n",
    "    \n",
    "    # If all of last n frames had train detected on at least one side\n",
    "    if left_cum >= history_length or right_cum >= history_length: \n",
    "        \n",
    "        # If a train was detected longer on the left, then it is south bound\n",
    "        if left_cum > right_cum:\n",
    "            print 'South bound train beginning at frame', j-history_length\n",
    "        else: \n",
    "            print 'North bound train beginning at frame', j-history_length\n",
    "        \n",
    "        train = history.tail(history_length).head(10)\n",
    "        \n",
    "        # Reset the history to be able to detect a new train\n",
    "        history = pd.DataFrame(columns=['Detected left','Detected right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real-time deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Raspberry pi setup\n",
    "Cost ~ $130\n",
    "<center><img src=\"figures/pi-hardware.png\" alt=\"Tweent\" width=\"900\" style=\"horizontal-align:middle\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Detecting motion at night </h1></center>\n",
    "<center><img src=\"figures/infra.jpg\" alt=\"Infrared photography\" width=\"450\" style=\"horizontal-align:middle\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h1>Detecting motion at night </h1></center>\n",
    "<center><img src=\"figures/infra.jpg\" alt=\"Infrared photography\" width=\"450\" style=\"horizontal-align:middle\"></center>\n",
    "<center>Photography from http://www.david-keochkerian.com/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1> PiCamera python package </h1>\n",
    "\n",
    "<p>\n",
    "```python\n",
    "\n",
    "import time\n",
    "import picamera\n",
    "import picamera.array\n",
    "import cv2\n",
    "\n",
    "\n",
    "camera = picamera.PiCamera()\n",
    "\n",
    "# Adjust resolution of the camera\n",
    "camera.resolution = (1024, 768)\n",
    "\n",
    "# Adjust framerate of the camera \n",
    "camera.framerate = 30 # frames per second\n",
    "\n",
    "# Save current frame to jpg\n",
    "camera.capture('image-file.jpg')\n",
    "\n",
    "# Start live streaming of video\n",
    "camera.start_preview()\n",
    "\n",
    "# Flip the camera\n",
    "camera.vflip = True\n",
    "camera.hflip = True\n",
    "\n",
    "# Set the brightness and contrast \n",
    "camera.brightness = 60\n",
    "camera.contrast = 30\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing the algorithm\n",
    "\n",
    "<p>\n",
    "```python\n",
    "with picamera.PiCamera() as camera:\n",
    "    \n",
    "    camera.start_preview()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    with picamera.array.PiRGBArray(camera) as stream:\n",
    "        \n",
    "        camera.capture(stream, format='bgr')\n",
    "        \n",
    "        frame = stream.array\n",
    "        \n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Processing done on the pi to reduce storage needs\n",
    "* Processed signals sent back to Grafana\n",
    "* Second signal of microphone for redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video quality considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "May want to reduce frame rate and resolution to be able to run on Raspberry pi --> will need to change parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap of parameters\n",
    "* `k` : Size of the kernel used for Gaussian blur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `area_thresh` : Minimum fraction of ROI that must be \"in motion\" to be considered train-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `alpha` : How heavily to weigh each new frame in running average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `history_length` : Number of frames in motion required to be considered a train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.mlab import bivariate_normal\n",
    "\n",
    "X, Y = np.mgrid[-31:31, -31:31]\n",
    "\n",
    "Z1 = bivariate_normal(X, Y, 5, 5) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9.5,8))\n",
    "plt.pcolor(X, Y, Z1);\n",
    "plt.colorbar();\n",
    "\n",
    "plt.xlim([-30, 30]);\n",
    "plt.ylim([-30, 30]);\n",
    "plt.title('Gaussian with $k_x = k_y = 31$', color='Black'); \n",
    "plt.savefig('figures/gaussian-color-k.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resolution + `k`\n",
    "\n",
    "Change `k` proportionally with the resolution.\n",
    "\n",
    "<img src=\"figures/gaussian-color-k.jpg\" alt=\"Gaussian\" width=\"540\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Kernel size determines neighborhood to use in computing each pixel\n",
    "* Want to scale that neighborhood with the size of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Rescaling the frames to 1/8 resolution and reducing number of frames by half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "r = 80.0 / original[0].shape[1]\n",
    "dim = (80, int(original[0].shape[0] * r))\n",
    "resized = []\n",
    "for j, frame in enumerate(original):\n",
    "    resized_frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    if j%2 == 0:\n",
    "        resized.append(resized_frame)\n",
    "        cv2.imshow('Resized', resized_frame)\n",
    "    \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "cv2.destroyWindow('Resized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Producing example figures of new kernel size for reduced resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(12,8))\n",
    "frame = original[20]\n",
    "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "resized_frame = resized[10]\n",
    "gray_resized = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "smooth31 = cv2.GaussianBlur(gray_frame, (31, 31), 0)\n",
    "plt.subplot(2,2,1);\n",
    "plt.imshow(gray_frame,cmap='Greys_r'); plt.axis('off'); \n",
    "plt.title('Original image');\n",
    "\n",
    "plt.subplot(2,2,2);\n",
    "plt.imshow(gray_resized, cmap='Greys_r'); plt.axis('off'); \n",
    "plt.title('1/8 resolution image');\n",
    "\n",
    "plt.subplot(2,2,3);\n",
    "plt.imshow(smooth31,cmap='Greys_r'); plt.axis('off'); \n",
    "plt.title('Original image, $k_x = k_y = 31$');\n",
    "smooth_resized = cv2.GaussianBlur(gray_resized, (3,3), 0)\n",
    "\n",
    "plt.subplot(2,2,4);\n",
    "plt.imshow(smooth_resized, cmap='Greys_r'); plt.axis('off'); \n",
    "plt.title('1/8 resolution image, $k_x = k_y = 3$');\n",
    "plt.tight_layout();\n",
    "plt.savefig('figures/sigmag-resolution-k.jpg')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h1>Resolution and <code>k</code></h1>\n",
    "</center>\n",
    "\n",
    "<center><img src='figures/sigmag-resolution-k.jpg' alt=\"Gaussian\" width=\"860\" align=\"center\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Proving algorithm on reduced resolution and frame reate and adjusted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "history_length = 20\n",
    "area_thresh = 0.025\n",
    "history = pd.DataFrame(columns=['Detected left','Detected right'])\n",
    "\n",
    "resized_left_fractions = []\n",
    "resized_right_fractions = []\n",
    "running_avg = None \n",
    "thresh = 35 \n",
    "alpha = 0.36\n",
    "k = 3\n",
    "\n",
    "for j, frame in enumerate(resized):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    if running_avg is None:\n",
    "        running_avg = np.float32(smooth_frame) \n",
    "    \n",
    "    diff = cv2.absdiff(np.float32(smooth_frame), np.float32(running_avg))\n",
    "    cv2.accumulateWeighted(np.float32(smooth_frame), running_avg, alpha)\n",
    "    _, subtracted = cv2.threshold(diff, thresh, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "    left_area = subtracted[34:50,30:42] \n",
    "    right_area = subtracted[34:50,68:80] \n",
    "        \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    resized_left_fractions.append(left_fraction)\n",
    "    resized_right_fractions.append(right_fraction)\n",
    "    # Update the history with whether the train detection criteria was met on either side\n",
    "    history.loc[len(history)+1] = [left_fraction>area_thresh, right_fraction>area_thresh]\n",
    "    \n",
    "    # Get how many of last n frames had a train detected for left and right ROIs\n",
    "    left_cum, right_cum = history.tail(history_length).sum()\n",
    "    \n",
    "    # If all of last n frames had train detected on at least one side\n",
    "    if left_cum >= history_length or right_cum >= history_length: \n",
    "        \n",
    "        # If a train was detected longer on the left, then it is south bound\n",
    "        if left_cum > right_cum:\n",
    "            print 'South bound train beginning at frame', j-history_length\n",
    "        else: \n",
    "            print 'North bound train beginning at frame', j-history_length\n",
    "        \n",
    "        train = history.tail(history_length).head(10)\n",
    "        \n",
    "        # Reset the history to be able to detect a new train\n",
    "        history = pd.DataFrame(columns=['Detected left','Detected right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Creating image for ROIs on reduced resolution image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "resized72 = cv2.resize(original[72], dim, interpolation = cv2.INTER_AREA)\n",
    "cv2.rectangle(resized72, (30,34), (42,50), (0, 204, 102), thickness=1)\n",
    "cv2.rectangle(resized72, (68,34), (80,50), (0, 85, 167), thickness=1)\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(resized72); plt.axis('off');\n",
    "plt.savefig('figures/resized-roi.jpg');\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Plotting fraction of ROIs in motion for new resolution/frame rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(left_fractions, color=colors[3], label='Left ROI');\n",
    "plt.plot(right_fractions, color=colors[0], label='Right ROI');\n",
    "plt.axhline(y=0.05, color=colors[1],label='area_thresh');\n",
    "plt.xlabel('Frame number');\n",
    "plt.ylabel('Fraction of ROI in motion');\n",
    "plt.title('Original');\n",
    "plt.legend(loc=2, fontsize=17);\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(resized_left_fractions, color=colors[3]);\n",
    "plt.plot(resized_right_fractions, color=colors[0]);\n",
    "plt.xlabel('Frame number');\n",
    "plt.title('1/8 resolution + 1/2 frame rate');\n",
    "plt.axhline(y=0.025, color=colors[1]);\n",
    "plt.savefig('figures/original-and-reduced-left-right-thresh.jpg')\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h1>Resolution + <code>area_thresh</code></h1></center>\n",
    "\n",
    "<center><img src=\"figures/resized-resized-roi.jpg\" alt=\"Resized ROIs\" width=\"300\" align=\"center\"></center>\n",
    "\n",
    "<center><img src=\"figures/original-and-reduced-left-right-thresh.jpg\" alt=\"Fraction in motion resized and original\" width=\"960\" align=\"center\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Frame rate + `alpha`\n",
    "* *Reminder*: `alpha` determines how heavily to weigh new frame in running average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* As frame rate (*fps*) is lowered, `alpha` should increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For $fps_{new} =\\frac{fps_{old}}{2}$, mathematically:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{new} = \\alpha_{old}(2-\\alpha_{old})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Get figure for slide to show history length change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(left_fractions);\n",
    "plt.axhline(y=.025,color=colors[3]);\n",
    "plt.axvline(x=35,color=colors[3], linestyle='--');\n",
    "plt.axvline(x=68,color=colors[3], linestyle='--');\n",
    "plt.xlim([0,160])\n",
    "plt.xlabel('Frame number');\n",
    "plt.ylabel('Fraction of frame in motion');\n",
    "plt.title('Fraction of frame in motion over time');\n",
    "plt.savefig('figures/short-frame-fraction-with-horizontal-and-vertical.jpg');\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h1>Frames rate + <code>history_length</code></h1></center>\n",
    "<center><img src=\"figures/green-frame-fraction-with-horizontal-and-vertical.jpg\" alt=\"Fraction in motion short\" width=\"860\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h1>Frames rate + <code>history_length</code></h1></center>\n",
    "<center><img src=\"figures/short-frame-fraction-with-horizontal-and-vertical.jpg\" alt=\"Fraction in motion short\" width=\"860\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Frame rate limitations\n",
    "Requires one frame with train present in only one ROI.\n",
    "\n",
    "<img src=\"figures/roiframe073.jpg\" alt=\"Fraction in motion\" width=\"760\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What if the light changes?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Background subtraction \n",
    "    * Frame differencing\n",
    "        * Good for indoors\n",
    "        * Very fast computationally\n",
    "        * Could train threshold for different lighting\n",
    "    * Alternative: *Adaptive background subtraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Camera settings\n",
    "    * Can adjust brightness and contrast of camera in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gaussian Mixture-based Background/Foreground Segmentation Algorithm (MOG, MOG2)\n",
    "\n",
    "* Each pixel considered separately\n",
    "* Background model (mixture of *k* Gaussian models) continually updated\n",
    "* Threshold is based on the pdf\n",
    " \n",
    "\n",
    "Implement in Python: \n",
    "\n",
    "`cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16,detectShadows=False)`\n",
    "\n",
    "* `history` = length of history to consider in describing the background model. \n",
    "* `varThreshold` = square of the number of standard deviations the pixel must be from its mean to be considered not background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Will work on straight color image but small scale movement will still likely show up (like leaves rustling)\n",
    "* Can using [morphological opening](https://en.wikipedia.org/wiki/Opening_(morphology) to remove areas too small to be considered motion for the application at hand. \n",
    "* After a time, the image will become the background, won't see end of train - need history to be much longer than length of event trying to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gaussian Mixture-based Background/Foreground Segmentation Algorithm (MOG, MOG2)\n",
    "\n",
    "```python\n",
    "mask = cv2.createBackgroundSubtractorMOG2(history=100, \n",
    "                                          varThreshold=25,\n",
    "                                          detectShadows=False)\n",
    "subtracted = frame.apply(mask)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><video width=\"640\" height=\"480\" align=\"center\" controls>\n",
    "  <source src=\"video/mog2.mp4\" type=\"video/mp4\">\n",
    "</video></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mask = cv2.createBackgroundSubtractorMOG2(history=100, \n",
    "                                          varThreshold=25,\n",
    "                                          detectShadows=False)\n",
    "\n",
    "history_length = 60\n",
    "area_thresh = 0.02\n",
    "history = pd.DataFrame(columns=['Detected left','Detected right'])\n",
    "left_fractions = []\n",
    "right_fractions = []\n",
    "running_avg = None \n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "\n",
    "extended_original = original[0:60]+original[60::-1]+original[0:60]+original[60::-1]+original\n",
    "\n",
    "for j, frame in enumerate(extended_original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    kernel = np.ones((11,11),np.uint8)\n",
    "    \n",
    "    subtracted = mask.apply(smooth_frame)\n",
    "    opening = cv2.morphologyEx(subtracted, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    left_area = opening[270:400,240:340]/255\n",
    "    right_area = opening[270:400,540:640]/255\n",
    "    \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    left_fractions.append(left_fraction)\n",
    "    right_fractions.append(right_fraction)\n",
    "    \n",
    "    # Update the history with whether the train detection criteria was met on either side\n",
    "    history.loc[len(history)+1] = [left_fraction>area_thresh, right_fraction>area_thresh]\n",
    "    \n",
    "    # Get how many of last n frames had a train detected for left and right ROIs\n",
    "    left_cum, right_cum = history.tail(history_length).sum()\n",
    "    \n",
    "    # If all of last n frames had train detected on at least one side\n",
    "    if left_cum >= history_length or right_cum >= history_length: \n",
    "        \n",
    "        # If a train was detected longer on the left, then it is south bound\n",
    "        if left_cum > right_cum:\n",
    "            print 'South bound train beginning at frame', j-history_length\n",
    "        else: \n",
    "            print 'North bound train beginning at frame', j-history_length\n",
    "        \n",
    "        train = history.tail(history_length).head(10)\n",
    "        \n",
    "        # Reset the history to be able to detect a new train\n",
    "        history = pd.DataFrame(columns=['Detected left','Detected right'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(left_fractions[-len(original):], label='Left ROI'); \n",
    "plt.plot(right_fractions[-len(original):], label='Right ROI'); \n",
    "plt.xlabel('Frame number'); plt.ylabel('Fraction of ROI in motion');\n",
    "plt.title('Fraction of ROIs in motion over time');plt.legend();\n",
    "plt.savefig('figures/left-right-fractions-mog2-extended.jpg');\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(left_fractions, label='Left ROI'); \n",
    "plt.plot(right_fractions, label='Right ROI'); \n",
    "plt.xlabel('Frame number'); plt.ylabel('Fraction of ROI in motion');\n",
    "plt.title('Fraction of ROIs in motion over time');plt.legend();\n",
    "plt.savefig('figures/left-right-fractions-mog2-original.jpg');\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"figures/left-right-fractions-mog2-100.jpg\" alt=\"MOG2 left right fractions\" width=\"660\" style=\"horizontal-align:middle\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Must increase the history length to detect longer last objects\n",
    "\n",
    "```python\n",
    "mask = cv2.createBackgroundSubtractorMOG2(history=400, \n",
    "                                          varThreshold=25,\n",
    "                                          detectShadows=False)\n",
    "subtracted = frame.apply(mask)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This time with a longer history to maintain detection \n",
    "# through length of train\n",
    "mask = cv2.createBackgroundSubtractorMOG2(history=400, \n",
    "                                          varThreshold=25,\n",
    "                                          detectShadows=False)\n",
    "\n",
    "history_length = 60\n",
    "area_thresh = 0.02\n",
    "history = pd.DataFrame(columns=['Detected left','Detected right'])\n",
    "left_fractions = []\n",
    "right_fractions = []\n",
    "running_avg = None \n",
    "thresh = 35 \n",
    "alpha = 0.2\n",
    "k = 31\n",
    "for j, frame in enumerate(original):\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     gray_frame = gamma_correction(gray_frame.astype('uint8'),0.75)\n",
    "    smooth_frame = cv2.GaussianBlur(gray_frame, (k, k), 0)\n",
    "    \n",
    "    kernel = np.ones((11,11),np.uint8)\n",
    "    \n",
    "    subtracted = mask.apply(smooth_frame)\n",
    "    opening = cv2.morphologyEx(subtracted, cv2.MORPH_OPEN, kernel)\n",
    "    cv2.imwrite('mog2'+str(j).zfill(3)+'.jpg',opening)\n",
    "    left_area = opening[270:400,240:340]/255\n",
    "    right_area = opening[270:400,540:640]/255\n",
    "    \n",
    "    left_fraction = sum(sum(left_area))*1.0/(left_area.shape[0]*left_area.shape[1])\n",
    "    right_fraction = sum(sum(right_area))*1.0/(right_area.shape[0]*right_area.shape[1])\n",
    "    \n",
    "    left_fractions.append(left_fraction)\n",
    "    right_fractions.append(right_fraction)\n",
    "    \n",
    "    # Update the history with whether the train detection criteria was met on either side\n",
    "    history.loc[len(history)+1] = [left_fraction>area_thresh, right_fraction>area_thresh]\n",
    "    \n",
    "    # Get how many of last n frames had a train detected for left and right ROIs\n",
    "    left_cum, right_cum = history.tail(history_length).sum()\n",
    "    \n",
    "    # If all of last n frames had train detected on at least one side\n",
    "    if left_cum >= history_length or right_cum >= history_length: \n",
    "        \n",
    "        # If a train was detected longer on the left, then it is south bound\n",
    "        if left_cum > right_cum:\n",
    "            print 'South bound train beginning at frame', j-history_length\n",
    "        else: \n",
    "            print 'North bound train beginning at frame', j-history_length\n",
    "        \n",
    "        train = history.tail(history_length).head(10)\n",
    "        \n",
    "        # Reset the history to be able to detect a new train\n",
    "        history = pd.DataFrame(columns=['Detected left','Detected right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/left-right-fractions-mog2-extended.jpg\" alt=\"MOG2 left right fractions\" width=\"660\" style=\"horizontal-align:middle\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h3>cmawer.github.io/trainspotting </h3></center>\n",
    "<center><img src=\"figures/thankyou.png\" alt=\"End\" width=\"500\"></center>\n",
    "<center><h3>@chloemawer | chloe@svds.com  </h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"figures/pi-arch.png\" alt=\"Tweent\" width=\"900\" style=\"horizontal-align:middle\"></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
